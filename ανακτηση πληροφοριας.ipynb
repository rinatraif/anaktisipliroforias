{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22d3c13-5a97-4133-82d0-0e1520311c3c",
   "metadata": {},
   "source": [
    "#Ανάκτηση Πληροφορίας\n",
    "#Εργαστηριακή Άσκηση Εξαμήνου"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b510bfe-5755-40d7-85aa-6261151103db",
   "metadata": {},
   "source": [
    "#Βήμα 1: Συλλογή δεδομένων από το Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb69b6be-c911-47e3-aa6e-8acd51fbaac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η σελίδα φορτώθηκε επιτυχώς!\n",
      "Τίτλος της σελίδας: Python (programming language)\n",
      "Μέρος του άρθρου:\n",
      "General-purpose programming language\n",
      "This article is about the Python programming language. For the animal, see Python (genus).\n",
      "\n",
      "\n",
      "PythonParadigmMulti-paradigm: object-oriented,[1] procedural (imperative), functional, structured, reflectiveDesigned byGuido van RossumDeveloperPython Software FoundationFirst appeared20 February 1991; 33 years ago (1991-02-20)[2]Stable release3.13.1\n",
      "   / 3 December 2024; 35 days ago (3 December 2024)\n",
      "Typing disciplineduck, dynamic, strong;[3] optional type annotatio\n",
      "Τα δεδομένα αποθηκεύτηκαν σε αρχείο 'wikipedia_article.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Βήμα 1: Διεύθυνση άρθρου στο Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "\n",
    "# Βήμα 2: Στείλε αίτημα HTTP για να κατεβάσεις το περιεχόμενο\n",
    "response = requests.get(url)\n",
    "\n",
    "# Έλεγχος ότι το αίτημα ήταν επιτυχές\n",
    "if response.status_code == 200:\n",
    "    print(\"Η σελίδα φορτώθηκε επιτυχώς!\")\n",
    "    \n",
    "    # Ανάλυση του HTML περιεχομένου με BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Βρες τον τίτλο του άρθρου\n",
    "    title = soup.find('h1').get_text()\n",
    "    print(f\"Τίτλος της σελίδας: {title}\")\n",
    "    \n",
    "    # Βρες το κυρίως περιεχόμενο του άρθρου\n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    if content:\n",
    "        text = content.get_text()\n",
    "        print(f\"Μέρος του άρθρου:\\n{text[:500]}\")  # Εκτύπωσε τους πρώτους 500 χαρακτήρες\n",
    "    else:\n",
    "        print(\"Δεν βρέθηκε περιεχόμενο στη σελίδα.\")\n",
    "else:\n",
    "    print(\"Σφάλμα κατά την φόρτωση της σελίδας.\")\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Βήμα 3: Αποθήκευση του άρθρου σε αρχείο JSON\n",
    "if content:\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"content\": text\n",
    "    }\n",
    "    # Αποθήκευση σε αρχείο JSON\n",
    "    with open('wikipedia_article.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(\"Τα δεδομένα αποθηκεύτηκαν σε αρχείο 'wikipedia_article.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831fd47-5312-4a5b-b5ee-cf05805a9cc3",
   "metadata": {},
   "source": [
    "#Βήμα 2: Προεπεξεργασία κειμένου (Text Processing)\n",
    "Αρχικά κάνουμε εγκατάσταση κάποιων βιβλιοθηκών για να μπορέσει να τρέξει το πρόγραμμα και στη συνέχεια εκτελούμε τον κώδικα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1809dff-e413-47c9-b8d6-6e366886fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "481d33d3-0cc4-4611-91d6-2a40a21252c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86e11eb-a093-473d-91c9-6ad98e896274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το κείμενο προεπεξεργάστηκε και αποθηκεύτηκε στο cleaned_wikipedia_article.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Εξασφαλίζουμε ότι έχουμε τα απαραίτητα δεδομένα από το nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Κάνουμε τα πάντα μικρά γράμματα\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Αφαίρεση μη-αλφαριθμητικών χαρακτήρων\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Διαχωρισμός σε λέξεις\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Αφαίρεση stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Χρήση Lemmatization για την επιστροφή των λέξεων στην κανονική τους μορφή\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Επιστροφή του επεξεργασμένου κειμένου\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_data():\n",
    "    with open('wikipedia_article.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Εξάγουμε το περιεχόμενο του άρθρου\n",
    "    article_text = data.get('content', '')\n",
    "\n",
    "    # Επεξεργαζόμαστε το κείμενο\n",
    "    cleaned_text = preprocess_text(article_text)\n",
    "\n",
    "    # Αποθήκευση του καθαρισμένου κειμένου σε νέο αρχείο\n",
    "    with open('cleaned_wikipedia_article.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump({'cleaned_content': cleaned_text}, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Το κείμενο προεπεξεργάστηκε και αποθηκεύτηκε στο cleaned_wikipedia_article.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8a9fb-19bb-4a48-b208-66df48ddf2b2",
   "metadata": {},
   "source": [
    "#Βήμα 3: Ευρετήριο\n",
    "Δημιουργούμε ένα inverted index με σκοπό την αποτελεσματική αντιστοίχηση όρων στα έγγραφα που εμφανίζονται"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5db538a-a8e6-4827-ac76-7bb9a8359319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο inverted_index.json!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Εξασφαλίζουμε ότι έχουμε τα απαραίτητα δεδομένα από το nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Βήμα 1: Φόρτωση των δεδομένων από το cleaned_data.json\n",
    "with open(\"cleaned_wikipedia_article.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "\n",
    "# Βήμα 2: Δημιουργία του ανεστραμμένου ευρετηρίου\n",
    "inverted_index = defaultdict(list)  # Χρησιμοποιούμε defaultdict για εύκολη εισαγωγή δεδομένων\n",
    "\n",
    "# Εξασφαλίζουμε ότι έχουμε τα stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for doc_id, content in cleaned_data.items():\n",
    "    # Χρησιμοποιούμε το word_tokenize για να διαχωρίσουμε σωστά τις λέξεις\n",
    "    words = word_tokenize(content.lower())  # Μετατρέπουμε σε μικρά γράμματα για ομοιομορφία\n",
    "    \n",
    "    # Αφαίρεση stopwords και μη αλφαριθμητικών χαρακτήρων\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Προσθήκη των λέξεων στο ανεστραμμένο ευρετήριο\n",
    "    for word in set(words):  # Χρησιμοποιούμε set για να αποθηκεύσουμε μόνο μοναδικές λέξεις\n",
    "        inverted_index[word].append(doc_id)\n",
    "\n",
    "# Βήμα 3: Αποθήκευση του ανεστραμμένου ευρετηρίου σε αρχείο JSON\n",
    "with open(\"inverted_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο inverted_index.json!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489207a-6af2-4cc3-9d71-111ae43a3f14",
   "metadata": {},
   "source": [
    "#Βήμα 4: Μηχανή αναζήτησης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cba1bd94-7f40-4f3d-b1fc-2abcb81d777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\catra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  Python programming\n",
      "Choose retrieval method (boolean/tfidf):  tfidf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results (TF-IDF):\n",
      "Document: cleaned_content | Score: -0.04757677666427172\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Εξασφαλίζουμε ότι έχουμε τα απαραίτητα δεδομένα από το nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Φόρτωση του ανεστραμμένου ευρετηρίου\n",
    "with open(\"inverted_index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "# Φόρτωση των εγγράφων\n",
    "with open(\"cleaned_wikipedia_article.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "\n",
    "# Λειτουργία Tokenization και καθαρισμού\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()  # Μετατρέπουμε το ερώτημα σε μικρά γράμματα\n",
    "    words = word_tokenize(query)  # Διαχωρίζουμε το ερώτημα σε λέξεις\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Boolean Retrieval (AND, OR, NOT)\n",
    "def boolean_retrieval(query, operator=\"AND\"):\n",
    "    query_terms = preprocess_query(query)\n",
    "    result_docs = set(cleaned_data.keys())\n",
    "    \n",
    "    if operator == \"AND\":\n",
    "        # Βρίσκουμε έγγραφα που περιέχουν όλα τα terms\n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                result_docs &= set(inverted_index[term])\n",
    "            else:\n",
    "                result_docs = set()  # Αν μια λέξη δεν υπάρχει, το αποτέλεσμα είναι κενό\n",
    "                break\n",
    "    elif operator == \"OR\":\n",
    "        # Βρίσκουμε έγγραφα που περιέχουν τουλάχιστον ένα από τα terms\n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                result_docs |= set(inverted_index[term])\n",
    "    elif operator == \"NOT\":\n",
    "        # Βρίσκουμε έγγραφα που δεν περιέχουν κάποια από τα terms\n",
    "        for term in query_terms:\n",
    "            if term in inverted_index:\n",
    "                result_docs -= set(inverted_index[term])\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "# TF-IDF Retrieval\n",
    "def tf_idf(query):\n",
    "    query_terms = preprocess_query(query)\n",
    "    doc_scores = defaultdict(float)\n",
    "\n",
    "    # Υπολογισμός TF για τα έγγραφα\n",
    "    for doc_id, content in cleaned_data.items():\n",
    "        term_freq = defaultdict(int)\n",
    "        words = word_tokenize(content.lower())\n",
    "        for word in words:\n",
    "            if word in query_terms:\n",
    "                term_freq[word] += 1\n",
    "\n",
    "        # Υπολογισμός του TF\n",
    "        for word, freq in term_freq.items():\n",
    "            tf = freq / len(words)\n",
    "            # Υπολογισμός του IDF\n",
    "            idf = math.log(len(cleaned_data) / (1 + len(inverted_index.get(word, []))))\n",
    "            doc_scores[doc_id] += tf * idf  # Συνδυασμός TF και IDF\n",
    "\n",
    "    # Ταξινόμηση των εγγράφων σύμφωνα με το σκορ TF-IDF\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "# Διεπαφή αναζήτησης\n",
    "def search(query, method=\"boolean\", operator=\"AND\"):\n",
    "    if method == \"boolean\":\n",
    "        return boolean_retrieval(query, operator)\n",
    "    elif method == \"tfidf\":\n",
    "        return tf_idf(query)\n",
    "    else:\n",
    "        print(\"Unknown method!\")\n",
    "        return []\n",
    "\n",
    "# Παράδειγμα χρήσης\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your query: \")\n",
    "    method = input(\"Choose retrieval method (boolean/tfidf): \").lower()\n",
    "    \n",
    "    if method == \"boolean\":\n",
    "        operator = input(\"Choose operator (AND/OR/NOT): \").upper()\n",
    "        results = search(query, method=method, operator=operator)\n",
    "    else:\n",
    "        results = search(query, method=method)\n",
    "    \n",
    "    if isinstance(results, list):\n",
    "        print(\"Results (TF-IDF):\")\n",
    "        for doc_id, score in results:\n",
    "            print(f\"Document: {doc_id} | Score: {score}\")\n",
    "    else:\n",
    "        print(\"Results (Boolean):\")\n",
    "        for doc_id in results:\n",
    "            print(f\"Document: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff774-f357-4e62-a3df-65849152ed97",
   "metadata": {},
   "source": [
    "#Βήμα 5: Αξιολόγηση Συστήματος"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "806a4704-2729-4eed-8003-34036ec2924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα Αξιολόγησης:\n",
      "Mean Precision: 0.67\n",
      "Mean Recall: 1.00\n",
      "Mean F1-Score: 0.78\n",
      "MAP: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Σύνολο δοκιμαστικών ερωτημάτων και σχετικών εγγράφων\n",
    "queries = {\n",
    "    \"Python programming\": [\"Python_(programming_language)\", \"Artificial_intelligence\"],\n",
    "    \"Machine learning\": [\"Machine_learning\"],\n",
    "    \"Artificial intelligence\": [\"Artificial_intelligence\"]\n",
    "}\n",
    "\n",
    "# Αποτελέσματα που ανακτήθηκαν από τη μηχανή αναζήτησης\n",
    "retrieved_results = {\n",
    "    \"Python programming\": [\"Python_(programming_language)\", \"Machine_learning\"],\n",
    "    \"Machine learning\": [\"Machine_learning\"],\n",
    "    \"Artificial intelligence\": [\"Artificial_intelligence\", \"Python_(programming_language)\"]\n",
    "}\n",
    "\n",
    "# Αξιολογήσεις σχετικότητας\n",
    "true_relevance = {\n",
    "    \"Python programming\": [1, 1],\n",
    "    \"Machine learning\": [1],\n",
    "    \"Artificial intelligence\": [1, 0]\n",
    "}\n",
    "\n",
    "# Υπολογισμός μετρικών αξιολόγησης\n",
    "def evaluate_search_engine(queries, retrieved_results, true_relevance):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    average_precisions = []\n",
    "\n",
    "    for query, relevant_docs in queries.items():\n",
    "        # Ανακτήθηκαν τα έγγραφα για το συγκεκριμένο ερώτημα\n",
    "        retrieved_docs = retrieved_results.get(query, [])\n",
    "\n",
    "        # Ετικέτες πραγματικής σχετικότητας\n",
    "        true_labels = [1 if doc in relevant_docs else 0 for doc in retrieved_docs]\n",
    "\n",
    "        # Υπολογισμός ακρίβειας και ανάκλησης\n",
    "        precisions.append(precision_score(true_labels, [1] * len(true_labels), zero_division=1))\n",
    "        recalls.append(recall_score(true_labels, [1] * len(true_labels), zero_division=1))\n",
    "        f1_scores.append(f1_score(true_labels, [1] * len(true_labels), zero_division=1))\n",
    "\n",
    "        # Υπολογισμός Mean Average Precision (MAP)\n",
    "        precision_at_k = [\n",
    "            precision_score(true_labels[:k], [1] * k, zero_division=1) for k in range(1, len(true_labels) + 1)\n",
    "        ]\n",
    "        average_precisions.append(np.mean(precision_at_k))\n",
    "\n",
    "    return {\n",
    "        \"Mean Precision\": np.mean(precisions),\n",
    "        \"Mean Recall\": np.mean(recalls),\n",
    "        \"Mean F1-Score\": np.mean(f1_scores),\n",
    "        \"MAP\": np.mean(average_precisions)\n",
    "    }\n",
    "\n",
    "# Εκτέλεση αξιολόγησης\n",
    "metrics = evaluate_search_engine(queries, retrieved_results, true_relevance)\n",
    "\n",
    "# Εκτύπωση αποτελεσμάτων\n",
    "print(\"Αποτελέσματα Αξιολόγησης:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2acea-1ede-47dd-8d05-b83101965271",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
